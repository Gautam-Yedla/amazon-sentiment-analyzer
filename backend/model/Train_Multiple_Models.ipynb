{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f232f222",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "import os\n",
    "from torch.utils.data import DataLoader, Dataset as TorchDataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b1bef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 1. Load and Split Data ===\n",
    "\n",
    "data_path = 'E:/amazon-sentiment-analyzer/backend/data/train_all_3class.csv'\n",
    "\n",
    "print(f\"ğŸ“‚ Loading dataset from: {data_path} ...\")\n",
    "df = pd.read_csv(data_path)\n",
    "print(f\"âœ… Loaded {len(df)} rows.\")\n",
    "\n",
    "# Show basic dataset info\n",
    "print(\"\\nğŸ“Š Label distribution BEFORE split:\")\n",
    "label_counts = Counter(df['label'])\n",
    "for label, count in sorted(label_counts.items()):\n",
    "    label_name = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}.get(label, str(label))\n",
    "    print(f\"  {label} ({label_name}): {count:,} samples\")\n",
    "\n",
    "# Split\n",
    "print(\"\\nâœ‚ï¸ Splitting dataset into Train and Test (80/20)...\")\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    df['text'], df['label'], test_size=0.2, random_state=42, stratify=df['label']\n",
    ")\n",
    "\n",
    "# Convert to DataFrames to view shape\n",
    "train_df = pd.DataFrame({'text': train_texts, 'label': train_labels})\n",
    "test_df = pd.DataFrame({'text': test_texts, 'label': test_labels})\n",
    "\n",
    "print(f\"\\nâœ… Split complete!\")\n",
    "print(f\"  â¤ Train set: {len(train_df)} samples\")\n",
    "print(f\"  â¤ Test set:  {len(test_df)} samples\")\n",
    "\n",
    "# Show post-split label distribution\n",
    "print(\"\\nğŸ“Š Label distribution AFTER split:\")\n",
    "for split_name, labels in [(\"Train\", train_labels), (\"Test\", test_labels)]:\n",
    "    counts = Counter(labels)\n",
    "    print(f\"  {split_name} set:\")\n",
    "    for label, count in sorted(counts.items()):\n",
    "        label_name = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}.get(label, str(label))\n",
    "        print(f\"    {label} ({label_name}): {count:,} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b9ba0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 2. TF-IDF + Classical ML Models ===\n",
    "\n",
    "print(\"\\nğŸ”§ Step 2: TF-IDF Vectorization + Classical ML Models\")\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"â³ Vectorizing text with TF-IDF (max_features=100000)...\")\n",
    "tfidf = TfidfVectorizer(max_features=100000)\n",
    "\n",
    "X_train_tfidf = tfidf.fit_transform(train_texts)\n",
    "X_test_tfidf = tfidf.transform(test_texts)\n",
    "\n",
    "print(f\"âœ… TF-IDF complete. Vector shapes: Train={X_train_tfidf.shape}, Test={X_test_tfidf.shape}\")\n",
    "print(f\"â±ï¸ Time taken: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=300, n_jobs=-1),\n",
    "    \"Naive Bayes\": MultinomialNB(),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, n_jobs=-1),\n",
    "    \"XGBoost\": XGBClassifier(n_estimators=100, use_label_encoder=False, eval_metric='mlogloss', verbosity=0),\n",
    "    \"LightGBM\": LGBMClassifier(n_estimators=100, verbosity=-1)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "print(\"\\nğŸš€ Training models...\")\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nğŸ“š Training: {name}\")\n",
    "    start = time.time()\n",
    "    model.fit(X_train_tfidf, train_labels)\n",
    "    preds = model.predict(X_test_tfidf)\n",
    "\n",
    "    acc = accuracy_score(test_labels, preds)\n",
    "    print(f\"ğŸ¯ Accuracy: {acc:.4f}\")\n",
    "    print(f\"ğŸ“‹ Classification Report:\\n{classification_report(test_labels, preds, target_names=['Negative', 'Neutral', 'Positive'])}\")\n",
    "    print(f\"â±ï¸ Time taken: {time.time() - start:.2f} sec\")\n",
    "\n",
    "    results[name] = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea40a788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "print(\"\\nğŸ“š Training: Logistic Regression\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Initialize and train the model\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "print(\"â³ Fitting model...\")\n",
    "lr.fit(X_train_tfidf, train_labels)\n",
    "\n",
    "# Predict\n",
    "print(\"ğŸ” Predicting on test set...\")\n",
    "y_pred_lr = lr.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate\n",
    "acc_lr = accuracy_score(test_labels, y_pred_lr)\n",
    "print(f\"âœ… Accuracy: {acc_lr:.4f}\")\n",
    "print(f\"ğŸ“‹ Classification Report:\\n{classification_report(test_labels, y_pred_lr, target_names=['Negative', 'Neutral', 'Positive'])}\")\n",
    "\n",
    "# Store result\n",
    "results['LogisticRegression'] = acc_lr\n",
    "\n",
    "print(f\"â±ï¸ Time taken: {time.time() - start_time:.2f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2868bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "print(\"\\nğŸŒ² Training: Random Forest\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Initialize and train the model\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "print(\"â³ Fitting model...\")\n",
    "rf.fit(X_train_tfidf, train_labels)\n",
    "\n",
    "# Predict\n",
    "print(\"ğŸ” Predicting on test set...\")\n",
    "y_pred_rf = rf.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate\n",
    "acc_rf = accuracy_score(test_labels, y_pred_rf)\n",
    "print(f\"âœ… Accuracy: {acc_rf:.4f}\")\n",
    "print(f\"ğŸ“‹ Classification Report:\\n{classification_report(test_labels, y_pred_rf, target_names=['Negative', 'Neutral', 'Positive'])}\")\n",
    "\n",
    "# Store result\n",
    "results['RandomForest'] = acc_rf\n",
    "\n",
    "print(f\"â±ï¸ Time taken: {time.time() - start_time:.2f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d74b18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "print(\"\\nâš¡ Training: XGBoost (GPU enabled)\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Initialize XGBoost model\n",
    "xgb_clf = XGBClassifier(\n",
    "    tree_method='gpu_hist',        # Use GPU\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='mlogloss',\n",
    "    random_state=42,\n",
    "    verbosity=1\n",
    ")\n",
    "\n",
    "print(\"â³ Fitting model...\")\n",
    "xgb_clf.fit(X_train_tfidf, train_labels)\n",
    "\n",
    "# Predict\n",
    "print(\"ğŸ” Predicting on test set...\")\n",
    "y_pred_xgb = xgb_clf.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate\n",
    "acc_xgb = accuracy_score(test_labels, y_pred_xgb)\n",
    "print(f\"âœ… Accuracy: {acc_xgb:.4f}\")\n",
    "print(f\"ğŸ“‹ Classification Report:\\n{classification_report(test_labels, y_pred_xgb, target_names=['Negative', 'Neutral', 'Positive'])}\")\n",
    "\n",
    "# Store result\n",
    "results['XGBoost'] = acc_xgb\n",
    "\n",
    "print(f\"â±ï¸ Time taken: {time.time() - start_time:.2f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85416d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM\n",
    "print(\"\\nâš¡ Training: LightGBM (GPU enabled)\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Initialize LightGBM model with GPU support\n",
    "lgb_clf = LGBMClassifier(\n",
    "    device='gpu',\n",
    "    boosting_type='gbdt',\n",
    "    objective='multiclass',\n",
    "    num_class=3,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"â³ Fitting model...\")\n",
    "lgb_clf.fit(X_train_tfidf, train_labels)\n",
    "\n",
    "# Predict\n",
    "print(\"ğŸ” Predicting on test set...\")\n",
    "y_pred_lgb = lgb_clf.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate\n",
    "acc_lgb = accuracy_score(test_labels, y_pred_lgb)\n",
    "print(f\"âœ… Accuracy: {acc_lgb:.4f}\")\n",
    "print(f\"ğŸ“‹ Classification Report:\\n{classification_report(test_labels, y_pred_lgb, target_names=['Negative', 'Neutral', 'Positive'])}\")\n",
    "\n",
    "# Store result\n",
    "results['LightGBM'] = acc_lgb\n",
    "\n",
    "print(f\"â±ï¸ Time taken: {time.time() - start_time:.2f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b2ffc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 3. LSTM with PyTorch ===\n",
    "print(\"\\nğŸ§  Training: LSTM with PyTorch\")\n",
    "\n",
    "# Dataset definition\n",
    "class TextDataset(TorchDataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.sequences = tokenizer.texts_to_sequences(texts)\n",
    "        self.sequences = pad_sequences(self.sequences, maxlen=max_len)\n",
    "        self.labels = torch.tensor(labels.values)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.sequences[idx], dtype=torch.long), self.labels[idx]\n",
    "\n",
    "# Hyperparameters\n",
    "max_len = 200\n",
    "vocab_size = 50000\n",
    "batch_size = 512\n",
    "embed_dim = 128\n",
    "hidden_dim = 64\n",
    "num_epochs = 3\n",
    "\n",
    "# Tokenization\n",
    "print(\"ğŸ”  Tokenizing and preparing sequences...\")\n",
    "tokenizer_lstm = Tokenizer(num_words=vocab_size)\n",
    "tokenizer_lstm.fit_on_texts(train_texts)\n",
    "\n",
    "# Dataset & Loader\n",
    "train_dataset = TextDataset(train_texts, train_labels, tokenizer_lstm, max_len)\n",
    "test_dataset = TextDataset(test_texts, test_labels, tokenizer_lstm, max_len)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# LSTM Model\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        _, (hidden, _) = self.lstm(embedded)\n",
    "        return self.fc(hidden[-1])\n",
    "\n",
    "model = LSTMClassifier(vocab_size=vocab_size, embed_dim=embed_dim, hidden_dim=hidden_dim).cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    print(f\"\\nğŸ“š Epoch {epoch + 1}/{num_epochs}\")\n",
    "    for batch_idx, (batch_x, batch_y) in enumerate(train_loader):\n",
    "        batch_x, batch_y = batch_x.cuda(), batch_y.cuda()\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if (batch_idx + 1) % 20 == 0:\n",
    "            print(f\"  â¤ Batch {batch_idx+1}/{len(train_loader)} | Loss: {running_loss / (batch_idx+1):.4f}\")\n",
    "\n",
    "# Evaluation\n",
    "print(\"\\nğŸ” Evaluating model on test set...\")\n",
    "model.eval()\n",
    "preds, actuals = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_x, batch_y in test_loader:\n",
    "        batch_x = batch_x.cuda()\n",
    "        outputs = model(batch_x)\n",
    "        preds.extend(outputs.argmax(1).cpu().numpy())\n",
    "        actuals.extend(batch_y.numpy())\n",
    "\n",
    "# Accuracy and report\n",
    "acc_lstm = accuracy_score(actuals, preds)\n",
    "print(f\"\\nâœ… Accuracy: {acc_lstm:.4f}\")\n",
    "print(f\"ğŸ“‹ Classification Report:\\n{classification_report(actuals, preds, target_names=['Negative', 'Neutral', 'Positive'])}\")\n",
    "\n",
    "results['LSTM'] = acc_lstm\n",
    "\n",
    "print(f\"â±ï¸ Total Time for LSTM: {time.time() - start_time:.2f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b67e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    BertTokenizer, BertForSequenceClassification,\n",
    "    TrainingArguments, Trainer\n",
    ")\n",
    "\n",
    "# === 1. Load Data ===\n",
    "data_path = \"E:/amazon-sentiment-analyzer/backend/data/train_all_3class.csv\"\n",
    "print(f\"ğŸ“‚ Loading dataset from: {data_path} ...\")\n",
    "df = pd.read_csv(data_path)\n",
    "print(f\"âœ… Loaded {len(df)} rows.\")\n",
    "\n",
    "# ğŸ§¼ Clean 'text' column\n",
    "print(\"\\nğŸ§¹ Cleaning text column...\")\n",
    "df = df.dropna(subset=[\"text\"])\n",
    "df[\"text\"] = df[\"text\"].astype(str)\n",
    "df = df[df[\"text\"].str.strip() != \"\"]  # remove empty strings\n",
    "print(f\"âœ… After cleaning: {len(df)} rows.\")\n",
    "\n",
    "# ğŸ“Š Label distribution\n",
    "print(\"\\nğŸ“Š Label distribution:\")\n",
    "label_counts = Counter(df[\"label\"])\n",
    "for label, count in sorted(label_counts.items()):\n",
    "    label_name = [\"Negative\", \"Neutral\", \"Positive\"][label]\n",
    "    print(f\"  {label} ({label_name}): {count} samples\")\n",
    "\n",
    "# === 2. Split into Train/Test ===\n",
    "print(\"\\nâœ‚ï¸ Splitting dataset into 80% train / 20% test ...\")\n",
    "train_df = df.sample(frac=0.8, random_state=42)\n",
    "test_df = df.drop(train_df.index)\n",
    "print(f\"âœ… Train size: {len(train_df)} | Test size: {len(test_df)}\")\n",
    "\n",
    "train_texts, train_labels = train_df[\"text\"].tolist(), train_df[\"label\"].tolist()\n",
    "test_texts, test_labels = test_df[\"text\"].tolist(), test_df[\"label\"].tolist()\n",
    "\n",
    "# === 3. Tokenizer ===\n",
    "print(\"\\nğŸ”  Loading BERT tokenizer (bert-base-uncased)...\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# === 4. Convert to HuggingFace Datasets ===\n",
    "print(\"ğŸ“¦ Converting to HuggingFace Dataset...\")\n",
    "train_ds = Dataset.from_dict({'text': train_texts, 'label': train_labels})\n",
    "test_ds = Dataset.from_dict({'text': test_texts, 'label': test_labels})\n",
    "print(f\"âœ… HuggingFace Datasets ready: Train={train_ds.num_rows}, Test={test_ds.num_rows}\")\n",
    "\n",
    "# === 5. Tokenization ===\n",
    "print(\"\\nğŸ§¼ Tokenizing datasets...\")\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(batch['text'], padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "train_ds = train_ds.map(tokenize_fn, batched=True)\n",
    "test_ds = test_ds.map(tokenize_fn, batched=True)\n",
    "train_ds.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "test_ds.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "print(\"âœ… Tokenization complete.\")\n",
    "\n",
    "# === 6. Load Model ===\n",
    "print(\"\\nğŸ§  Loading BERT model for classification...\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3).cuda()\n",
    "\n",
    "# === 7. Training Arguments ===\n",
    "print(\"âš™ï¸ Setting up training arguments...\")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"E:/amazon-sentiment-analyzer/backend/model/bert_output\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# === 8. Trainer Setup ===\n",
    "print(\"ğŸ‘¨â€ğŸ« Initializing Trainer...\")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# === 9. Training ===\n",
    "print(\"\\nğŸš€ Starting training...\")\n",
    "start_time = time.time()\n",
    "trainer.train()\n",
    "print(f\"âœ… Training done in {time.time() - start_time:.2f} seconds.\")\n",
    "\n",
    "# === 10. Evaluation ===\n",
    "print(\"\\nğŸ“Š Evaluating model on test set...\")\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"ğŸ“ˆ Evaluation Results:\")\n",
    "for k, v in eval_results.items():\n",
    "    print(f\"  {k}: {v:.4f}\")\n",
    "\n",
    "# === 11. Accuracy & Classification Report ===\n",
    "print(\"\\nğŸ” Running predictions on test set...\")\n",
    "preds = trainer.predict(test_ds).predictions\n",
    "preds_cls = torch.argmax(torch.tensor(preds), axis=1).numpy()\n",
    "\n",
    "acc = accuracy_score(test_labels, preds_cls)\n",
    "print(f\"\\nâœ… Test Accuracy: {acc:.4f}\")\n",
    "print(\"ğŸ“‹ Classification Report:\")\n",
    "print(classification_report(test_labels, preds_cls, target_names=[\"Negative\", \"Neutral\", \"Positive\"]))\n",
    "\n",
    "# === 12. Save Model and Tokenizer ===\n",
    "save_dir = \"E:/amazon-sentiment-analyzer/backend/model/bert_model\"\n",
    "print(f\"\\nğŸ’¾ Saving model & tokenizer to: {save_dir}\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "model.save_pretrained(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "print(\"âœ… Save complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b918fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # === 5. Compare All Models ===\n",
    "# print(\"\\nğŸ“Š Model Accuracies:\")\n",
    "# for name, acc in results.items():\n",
    "#     print(f\"  {name}: {acc:.4f}\")\n",
    "\n",
    "# best_model = max(results, key=results.get)\n",
    "# print(f\"\\nğŸ† Best model: {best_model} with accuracy {results[best_model]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f02fd11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ Loading val/test...\n",
      "ğŸ”  Fitting TF-IDF vectorizer on full training data...\n",
      "ğŸ§  Collected: 500,000 samples\n",
      "ğŸ§  Collected: 1,000,000 samples\n",
      "ğŸ§  Collected: 1,500,000 samples\n",
      "ğŸ§  Collected: 2,000,000 samples\n",
      "ğŸ§  Collected: 2,500,000 samples\n",
      "ğŸ§  Collected: 3,000,000 samples\n",
      "ğŸ§  Collected: 3,500,000 samples\n",
      "ğŸ§  Collected: 4,000,000 samples\n",
      "ğŸ§  Collected: 4,500,000 samples\n",
      "ğŸ§  Collected: 5,000,000 samples\n",
      "ğŸ§  Collected: 5,500,000 samples\n",
      "ğŸ§  Collected: 6,000,000 samples\n",
      "ğŸ§  Collected: 6,500,000 samples\n",
      "ğŸ§  Collected: 7,000,000 samples\n",
      "ğŸ§  Collected: 7,500,000 samples\n",
      "ğŸ§  Collected: 8,000,000 samples\n",
      "ğŸ§  Collected: 8,500,000 samples\n",
      "ğŸ§  Collected: 9,000,000 samples\n",
      "ğŸ§  Collected: 9,500,000 samples\n",
      "ğŸ§  Collected: 10,000,000 samples\n",
      "ğŸ§  Collected: 10,500,000 samples\n",
      "ğŸ§  Collected: 11,000,000 samples\n",
      "ğŸ§  Collected: 11,500,000 samples\n",
      "ğŸ§  Collected: 12,000,000 samples\n",
      "ğŸ§  Collected: 12,500,000 samples\n",
      "ğŸ§  Collected: 13,000,000 samples\n",
      "ğŸ§  Collected: 13,500,000 samples\n",
      "ğŸ§  Collected: 14,000,000 samples\n",
      "ğŸ§  Collected: 14,500,000 samples\n",
      "ğŸ§  Collected: 15,000,000 samples\n",
      "ğŸ§  Collected: 15,500,000 samples\n",
      "ğŸ§  Collected: 16,000,000 samples\n",
      "ğŸ§  Collected: 16,500,000 samples\n",
      "ğŸ§  Collected: 17,000,000 samples\n",
      "ğŸ§  Collected: 17,500,000 samples\n",
      "ğŸ§  Collected: 18,000,000 samples\n",
      "ğŸ§  Collected: 18,500,000 samples\n",
      "ğŸ§  Collected: 19,000,000 samples\n",
      "ğŸ§  Collected: 19,129,109 samples\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import os\n",
    "import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from scipy.sparse import vstack\n",
    "\n",
    "# === Paths ===\n",
    "data_dir = \"E:/amazon-sentiment-analyzer/backend/data/\"\n",
    "train_path = os.path.join(data_dir, \"train01.csv\")\n",
    "val_path = os.path.join(data_dir, \"val01.csv\")\n",
    "test_path = os.path.join(data_dir, \"test01.csv\")\n",
    "\n",
    "CHUNKSIZE = 500_000  # Adjust based on RAM\n",
    "\n",
    "print(\"ğŸ“‚ Loading val/test...\")\n",
    "val_df = pd.read_csv(val_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "val_df['text'] = val_df['text'].fillna(\"\").astype(str)\n",
    "test_df['text'] = test_df['text'].fillna(\"\").astype(str)\n",
    "\n",
    "# === Fit TF-IDF on chunks from training set ===\n",
    "print(\"ğŸ”  Fitting TF-IDF vectorizer on full training data...\")\n",
    "vectorizer = TfidfVectorizer(max_features=100_000)\n",
    "\n",
    "train_texts = []\n",
    "reader = pd.read_csv(train_path, chunksize=CHUNKSIZE)\n",
    "for chunk in reader:\n",
    "    chunk['text'] = chunk['text'].fillna(\"\").astype(str)\n",
    "    train_texts.extend(chunk['text'].tolist())\n",
    "    print(f\"ğŸ§  Collected: {len(train_texts):,} samples\")\n",
    "\n",
    "vectorizer.fit(train_texts)\n",
    "print(\"âœ… TF-IDF fit complete.\")\n",
    "\n",
    "# === Convert all train data to vectors (in chunks) ===\n",
    "print(\"ğŸ”„ Transforming train data to vectors...\")\n",
    "X_train_chunks = []\n",
    "y_train_chunks = []\n",
    "\n",
    "reader = pd.read_csv(train_path, chunksize=CHUNKSIZE)\n",
    "for chunk in reader:\n",
    "    chunk['text'] = chunk['text'].fillna(\"\").astype(str)\n",
    "    X_chunk = vectorizer.transform(chunk['text'])\n",
    "    X_train_chunks.append(X_chunk)\n",
    "    y_train_chunks.extend(chunk['label'].tolist())\n",
    "    print(f\"ğŸ“¦ Processed chunk with {len(chunk):,} rows\")\n",
    "\n",
    "X_train = vstack(X_train_chunks)\n",
    "y_train = y_train_chunks\n",
    "\n",
    "print(f\"âœ… Final X_train shape: {X_train.shape}\")\n",
    "\n",
    "# === Transform val/test ===\n",
    "X_val = vectorizer.transform(val_df['text'])\n",
    "X_test = vectorizer.transform(test_df['text'])\n",
    "y_val = val_df['label']\n",
    "y_test = test_df['label']\n",
    "\n",
    "# === Train Models ===\n",
    "models = {\n",
    "    \"LogisticRegression\": LogisticRegression(max_iter=200, solver='saga', n_jobs=-1),\n",
    "    \"LightGBM\": LGBMClassifier(n_estimators=300, learning_rate=0.1, num_leaves=64, n_jobs=-1),\n",
    "    \"XGBoost\": XGBClassifier(n_estimators=300, learning_rate=0.1, max_depth=6, n_jobs=-1, tree_method='gpu_hist')\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nğŸš€ Training: {name}\")\n",
    "    start = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    train_time = time.time() - start\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"âœ… {name} Accuracy: {acc:.4f} | Time: {train_time:.2f}s\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    model_dir = f\"E:/amazon-sentiment-analyzer/backend/model/{name.lower()}_model\"\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    joblib.dump(model, os.path.join(model_dir, \"model.pkl\"))\n",
    "    joblib.dump(vectorizer, os.path.join(model_dir, \"vectorizer.pkl\"))\n",
    "    print(f\"ğŸ’¾ Saved model to {model_dir}\")\n",
    "\n",
    "    results[name] = {\"accuracy\": acc, \"time\": train_time}\n",
    "\n",
    "# === Summary ===\n",
    "print(\"\\nğŸ“Š Summary of Results:\")\n",
    "for name, metrics in results.items():\n",
    "    print(f\"{name}: Accuracy={metrics['accuracy']:.4f}, Time={metrics['time']:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbb7685a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ Loading val/test...\n",
      "ğŸ”  Fitting TF-IDF vectorizer...\n",
      "âœ… TF-IDF fit complete.\n",
      "ğŸ”„ Transforming train data...\n",
      "ğŸ“¦ Processed chunk 1 in 29.12s\n",
      "ğŸ“¦ Processed chunk 2 in 24.26s\n",
      "ğŸ“¦ Processed chunk 3 in 22.25s\n",
      "ğŸ“¦ Processed chunk 4 in 24.77s\n",
      "ğŸ“¦ Processed chunk 5 in 23.28s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 44\u001b[39m\n\u001b[32m     42\u001b[39m start = time.time()\n\u001b[32m     43\u001b[39m chunk[\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m] = chunk[\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m].fillna(\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m).astype(\u001b[38;5;28mstr\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m X_chunk = \u001b[43mvectorizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m X_train_chunks.append(X_chunk)\n\u001b[32m     46\u001b[39m y_train_chunks.extend(chunk[\u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m].tolist())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\Program Files\\Python\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2128\u001b[39m, in \u001b[36mTfidfVectorizer.transform\u001b[39m\u001b[34m(self, raw_documents)\u001b[39m\n\u001b[32m   2111\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Transform documents to document-term matrix.\u001b[39;00m\n\u001b[32m   2112\u001b[39m \n\u001b[32m   2113\u001b[39m \u001b[33;03mUses the vocabulary and document frequencies (df) learned by fit (or\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2124\u001b[39m \u001b[33;03m    Tf-idf-weighted document-term matrix.\u001b[39;00m\n\u001b[32m   2125\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2126\u001b[39m check_is_fitted(\u001b[38;5;28mself\u001b[39m, msg=\u001b[33m\"\u001b[39m\u001b[33mThe TF-IDF vectorizer is not fitted\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2128\u001b[39m X = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2129\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tfidf.transform(X, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\Program Files\\Python\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1421\u001b[39m, in \u001b[36mCountVectorizer.transform\u001b[39m\u001b[34m(self, raw_documents)\u001b[39m\n\u001b[32m   1418\u001b[39m \u001b[38;5;28mself\u001b[39m._check_vocabulary()\n\u001b[32m   1420\u001b[39m \u001b[38;5;66;03m# use the same matrix-building strategy as fit_transform\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1421\u001b[39m _, X = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfixed_vocab\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   1422\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.binary:\n\u001b[32m   1423\u001b[39m     X.data.fill(\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\Program Files\\Python\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1263\u001b[39m, in \u001b[36mCountVectorizer._count_vocab\u001b[39m\u001b[34m(self, raw_documents, fixed_vocab)\u001b[39m\n\u001b[32m   1261\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[32m   1262\u001b[39m     feature_counter = {}\n\u001b[32m-> \u001b[39m\u001b[32m1263\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   1264\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1265\u001b[39m             feature_idx = vocabulary[feature]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\Program Files\\Python\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:111\u001b[39m, in \u001b[36m_analyze\u001b[39m\u001b[34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[39m\n\u001b[32m    109\u001b[39m             doc = ngrams(doc, stop_words)\n\u001b[32m    110\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m             doc = \u001b[43mngrams\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m doc\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\Program Files\\Python\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:238\u001b[39m, in \u001b[36m_VectorizerMixin._word_ngrams\u001b[39m\u001b[34m(self, tokens, stop_words)\u001b[39m\n\u001b[32m    232\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    233\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mnp.nan is an invalid document, expected byte or unicode string.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    234\u001b[39m         )\n\u001b[32m    236\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m doc\n\u001b[32m--> \u001b[39m\u001b[32m238\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_word_ngrams\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokens, stop_words=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    239\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Turn tokens into a sequence of n-grams after stop words filtering\"\"\"\u001b[39;00m\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# handle stop words\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import os\n",
    "import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from scipy.sparse import vstack\n",
    "\n",
    "# === Paths ===\n",
    "data_dir = \"E:/amazon-sentiment-analyzer/backend/data/\"\n",
    "train_path = os.path.join(data_dir, \"train01.csv\")\n",
    "val_path = os.path.join(data_dir, \"val01.csv\")\n",
    "test_path = os.path.join(data_dir, \"test01.csv\")\n",
    "CHUNKSIZE = 500_000\n",
    "\n",
    "# === Load val/test ===\n",
    "print(\"ğŸ“‚ Loading val/test...\")\n",
    "val_df = pd.read_csv(val_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "val_df['text'] = val_df['text'].fillna(\"\").astype(str)\n",
    "test_df['text'] = test_df['text'].fillna(\"\").astype(str)\n",
    "\n",
    "# === Fit TF-IDF ===\n",
    "print(\"ğŸ”  Fitting TF-IDF vectorizer...\")\n",
    "vectorizer = TfidfVectorizer(max_features=50_000)\n",
    "train_texts = []\n",
    "reader = pd.read_csv(train_path, chunksize=CHUNKSIZE, nrows=100_000)  # Subsample for fitting\n",
    "for chunk in reader:\n",
    "    chunk['text'] = chunk['text'].fillna(\"\").astype(str)\n",
    "    train_texts.extend(chunk['text'].tolist())\n",
    "vectorizer.fit(train_texts)\n",
    "print(\"âœ… TF-IDF fit complete.\")\n",
    "\n",
    "# === Transform train data ===\n",
    "print(\"ğŸ”„ Transforming train data...\")\n",
    "X_train_chunks = []\n",
    "y_train_chunks = []\n",
    "reader = pd.read_csv(train_path, chunksize=CHUNKSIZE)\n",
    "for i, chunk in enumerate(reader):\n",
    "    start = time.time()\n",
    "    chunk['text'] = chunk['text'].fillna(\"\").astype(str)\n",
    "    X_chunk = vectorizer.transform(chunk['text'])\n",
    "    X_train_chunks.append(X_chunk)\n",
    "    y_train_chunks.extend(chunk['label'].tolist())\n",
    "    print(f\"ğŸ“¦ Processed chunk {i+1} in {time.time() - start:.2f}s\")\n",
    "X_train = vstack(X_train_chunks)\n",
    "y_train = y_train_chunks\n",
    "print(f\"âœ… X_train shape: {X_train.shape}\")\n",
    "\n",
    "# === Transform val/test ===\n",
    "X_val = vectorizer.transform(val_df['text'])\n",
    "X_test = vectorizer.transform(test_df['text'])\n",
    "y_val = val_df['label']\n",
    "y_test = test_df['label']\n",
    "\n",
    "# === Train Models ===\n",
    "models = {\n",
    "    \"SGDClassifier\": SGDClassifier(loss='log', max_iter=50, n_jobs=-1),\n",
    "    \"LightGBM\": LGBMClassifier(n_estimators=100, learning_rate=0.1, num_leaves=31, n_jobs=-1),\n",
    "    \"XGBoost\": XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=4, n_jobs=-1, tree_method='gpu_hist')\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nğŸš€ Training: {name}\")\n",
    "    start = time.time()\n",
    "    if name in [\"LightGBM\", \"XGBoost\"]:\n",
    "        model.fit(X_train, y_train, eval_set=[(X_val, y_val)], eval_metric='logloss', early_stopping_rounds=10)\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "    train_time = time.time() - start\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"âœ… {name} Accuracy: {acc:.4f} | Time: {train_time:.2f}s\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    model_dir = f\"E:/amazon-sentiment-analyzer/backend/model/{name.lower()}_model\"\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    joblib.dump(model, os.path.join(model_dir, \"model.pkl\"), compress=3)\n",
    "    joblib.dump(vectorizer, os.path.join(model_dir, \"vectorizer.pkl\"), compress=3)\n",
    "    results[name] = {\"accuracy\": acc, \"time\": train_time}\n",
    "\n",
    "# === Summary ===\n",
    "print(\"\\nğŸ“Š Summary of Results:\")\n",
    "sorted_results = sorted(results.items(), key=lambda x: x[1]['accuracy'], reverse=True)\n",
    "for name, metrics in sorted_results:\n",
    "    print(f\"{name}: Accuracy={metrics['accuracy']:.4f}, Time={metrics['time']:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7812581a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ scikit-learn version: 1.6.1\n",
      "ğŸ“‚ Loading val/test...\n",
      "Memory usage after loading: 2520.18 MB\n",
      "ğŸ” Computing class weights from sample...\n",
      "Unique labels in sample: [0 1 2 3 4]\n",
      "Class weights: {np.int64(0): np.float64(3.2229473853839337), np.int64(1): np.float64(4.4692737430167595), np.int64(2): np.float64(2.3066720489014476), np.int64(3): np.float64(1.1158534884369682), np.int64(4): np.float64(0.318849590676838)}\n",
      "ğŸ”  Initializing HashingVectorizer...\n",
      "âœ… Vectorizer ready.\n",
      "ğŸš€ Training SGDClassifier on chunks...\n",
      "ğŸ“¦ Epoch 1, Chunk 1 with 100,000 rows in 4.90s\n",
      "Memory usage: 2005.37 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 2 with 100,000 rows in 3.29s\n",
      "Memory usage: 1906.19 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 3 with 100,000 rows in 2.55s\n",
      "Memory usage: 1910.76 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 4 with 100,000 rows in 2.75s\n",
      "Memory usage: 1912.59 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 5 with 100,000 rows in 2.40s\n",
      "Memory usage: 1908.84 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 6 with 100,000 rows in 2.72s\n",
      "Memory usage: 1906.91 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 7 with 100,000 rows in 2.57s\n",
      "Memory usage: 1907.55 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 8 with 100,000 rows in 2.55s\n",
      "Memory usage: 1908.35 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 9 with 100,000 rows in 2.86s\n",
      "Memory usage: 1911.07 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 10 with 100,000 rows in 2.56s\n",
      "Memory usage: 1909.15 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 11 with 100,000 rows in 2.45s\n",
      "Memory usage: 1907.16 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 12 with 100,000 rows in 2.40s\n",
      "Memory usage: 1907.50 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 13 with 100,000 rows in 2.60s\n",
      "Memory usage: 1907.12 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 14 with 100,000 rows in 2.26s\n",
      "Memory usage: 1906.18 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 15 with 100,000 rows in 2.38s\n",
      "Memory usage: 1906.69 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 16 with 100,000 rows in 2.26s\n",
      "Memory usage: 1907.79 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 17 with 100,000 rows in 2.45s\n",
      "Memory usage: 1908.08 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 18 with 100,000 rows in 2.30s\n",
      "Memory usage: 1907.58 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 19 with 100,000 rows in 2.76s\n",
      "Memory usage: 1910.68 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 20 with 100,000 rows in 2.23s\n",
      "Memory usage: 1908.12 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 21 with 100,000 rows in 2.51s\n",
      "Memory usage: 1909.08 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 22 with 100,000 rows in 2.38s\n",
      "Memory usage: 1909.00 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 23 with 100,000 rows in 2.35s\n",
      "Memory usage: 1908.29 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 24 with 100,000 rows in 2.40s\n",
      "Memory usage: 1908.97 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 25 with 100,000 rows in 2.31s\n",
      "Memory usage: 1908.33 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 26 with 100,000 rows in 2.47s\n",
      "Memory usage: 1910.08 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 27 with 100,000 rows in 2.54s\n",
      "Memory usage: 1910.90 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 28 with 100,000 rows in 2.27s\n",
      "Memory usage: 1908.84 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 29 with 100,000 rows in 2.51s\n",
      "Memory usage: 1910.17 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 30 with 100,000 rows in 2.79s\n",
      "Memory usage: 1908.84 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 31 with 100,000 rows in 2.54s\n",
      "Memory usage: 1908.88 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 32 with 100,000 rows in 2.52s\n",
      "Memory usage: 1907.12 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 33 with 100,000 rows in 2.47s\n",
      "Memory usage: 1907.41 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 34 with 100,000 rows in 2.31s\n",
      "Memory usage: 1907.48 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 35 with 100,000 rows in 2.36s\n",
      "Memory usage: 1908.41 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 36 with 100,000 rows in 2.40s\n",
      "Memory usage: 1909.06 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 37 with 100,000 rows in 2.39s\n",
      "Memory usage: 1908.79 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 38 with 100,000 rows in 2.38s\n",
      "Memory usage: 1908.76 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 39 with 100,000 rows in 2.35s\n",
      "Memory usage: 1908.62 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 40 with 100,000 rows in 2.55s\n",
      "Memory usage: 1908.54 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 41 with 100,000 rows in 2.50s\n",
      "Memory usage: 1882.18 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 42 with 100,000 rows in 2.46s\n",
      "Memory usage: 1883.06 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 43 with 100,000 rows in 2.24s\n",
      "Memory usage: 1880.68 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 44 with 100,000 rows in 2.42s\n",
      "Memory usage: 1881.72 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 45 with 100,000 rows in 3.09s\n",
      "Memory usage: 1879.80 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 46 with 100,000 rows in 2.99s\n",
      "Memory usage: 1877.76 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 47 with 100,000 rows in 2.72s\n",
      "Memory usage: 1884.41 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 48 with 100,000 rows in 2.70s\n",
      "Memory usage: 1884.40 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 49 with 100,000 rows in 2.84s\n",
      "Memory usage: 1885.11 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 50 with 100,000 rows in 2.29s\n",
      "Memory usage: 1881.68 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 51 with 100,000 rows in 2.37s\n",
      "Memory usage: 1883.26 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 52 with 100,000 rows in 2.23s\n",
      "Memory usage: 1882.87 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 53 with 100,000 rows in 2.37s\n",
      "Memory usage: 1882.89 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 54 with 100,000 rows in 2.23s\n",
      "Memory usage: 1881.82 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 55 with 100,000 rows in 2.37s\n",
      "Memory usage: 1881.69 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 56 with 100,000 rows in 2.21s\n",
      "Memory usage: 1882.82 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 57 with 100,000 rows in 2.47s\n",
      "Memory usage: 1882.14 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 58 with 100,000 rows in 2.19s\n",
      "Memory usage: 1881.67 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 59 with 100,000 rows in 2.33s\n",
      "Memory usage: 1882.81 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 60 with 100,000 rows in 2.22s\n",
      "Memory usage: 1881.62 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 61 with 100,000 rows in 2.27s\n",
      "Memory usage: 1881.13 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 62 with 100,000 rows in 2.20s\n",
      "Memory usage: 1881.52 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 63 with 100,000 rows in 2.31s\n",
      "Memory usage: 1881.77 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 64 with 100,000 rows in 2.06s\n",
      "Memory usage: 1819.84 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 65 with 100,000 rows in 2.27s\n",
      "Memory usage: 1820.77 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 66 with 100,000 rows in 2.08s\n",
      "Memory usage: 1819.84 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 67 with 100,000 rows in 2.11s\n",
      "Memory usage: 1821.39 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 68 with 100,000 rows in 2.26s\n",
      "Memory usage: 1820.71 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 69 with 100,000 rows in 2.25s\n",
      "Memory usage: 1820.63 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 70 with 100,000 rows in 2.25s\n",
      "Memory usage: 1820.70 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 71 with 100,000 rows in 2.23s\n",
      "Memory usage: 1821.26 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 72 with 100,000 rows in 2.29s\n",
      "Memory usage: 1820.94 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 73 with 100,000 rows in 2.20s\n",
      "Memory usage: 1820.82 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 74 with 100,000 rows in 2.26s\n",
      "Memory usage: 1820.76 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 75 with 100,000 rows in 2.23s\n",
      "Memory usage: 1821.10 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 76 with 100,000 rows in 2.26s\n",
      "Memory usage: 1820.88 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 77 with 100,000 rows in 2.26s\n",
      "Memory usage: 1821.30 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 78 with 100,000 rows in 2.27s\n",
      "Memory usage: 1821.28 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 79 with 100,000 rows in 2.13s\n",
      "Memory usage: 1820.86 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 80 with 100,000 rows in 2.19s\n",
      "Memory usage: 1820.89 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 81 with 100,000 rows in 2.24s\n",
      "Memory usage: 1822.61 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 82 with 100,000 rows in 3.71s\n",
      "Memory usage: 1826.82 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 83 with 100,000 rows in 2.24s\n",
      "Memory usage: 1819.85 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 84 with 100,000 rows in 2.34s\n",
      "Memory usage: 1819.36 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 85 with 100,000 rows in 2.29s\n",
      "Memory usage: 1819.99 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 86 with 100,000 rows in 2.33s\n",
      "Memory usage: 1819.75 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 87 with 100,000 rows in 2.20s\n",
      "Memory usage: 1819.26 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 88 with 100,000 rows in 2.33s\n",
      "Memory usage: 1820.60 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 89 with 100,000 rows in 2.15s\n",
      "Memory usage: 1819.32 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 90 with 100,000 rows in 2.24s\n",
      "Memory usage: 1819.22 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 91 with 100,000 rows in 2.00s\n",
      "Memory usage: 1819.41 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 92 with 100,000 rows in 2.10s\n",
      "Memory usage: 1818.77 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 93 with 100,000 rows in 2.17s\n",
      "Memory usage: 1819.18 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 94 with 100,000 rows in 2.35s\n",
      "Memory usage: 1819.80 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 95 with 100,000 rows in 2.04s\n",
      "Memory usage: 1818.73 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 96 with 100,000 rows in 2.31s\n",
      "Memory usage: 1691.16 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 97 with 100,000 rows in 2.21s\n",
      "Memory usage: 1691.50 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 98 with 100,000 rows in 2.40s\n",
      "Memory usage: 1692.62 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 99 with 100,000 rows in 2.31s\n",
      "Memory usage: 1692.40 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 100 with 100,000 rows in 2.25s\n",
      "Memory usage: 1691.46 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 101 with 100,000 rows in 2.26s\n",
      "Memory usage: 1691.65 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 102 with 100,000 rows in 2.08s\n",
      "Memory usage: 1691.00 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 103 with 100,000 rows in 2.33s\n",
      "Memory usage: 1440.21 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 104 with 100,000 rows in 2.22s\n",
      "Memory usage: 1440.60 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 105 with 100,000 rows in 2.12s\n",
      "Memory usage: 1439.43 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 106 with 100,000 rows in 2.13s\n",
      "Memory usage: 1440.94 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 107 with 100,000 rows in 2.30s\n",
      "Memory usage: 1440.38 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 108 with 100,000 rows in 2.23s\n",
      "Memory usage: 1440.34 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 109 with 100,000 rows in 2.42s\n",
      "Memory usage: 1440.79 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 110 with 100,000 rows in 2.13s\n",
      "Memory usage: 1440.83 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 111 with 100,000 rows in 2.23s\n",
      "Memory usage: 1441.07 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 112 with 100,000 rows in 2.10s\n",
      "Memory usage: 1439.89 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 113 with 100,000 rows in 2.30s\n",
      "Memory usage: 1441.19 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 114 with 100,000 rows in 2.12s\n",
      "Memory usage: 1439.96 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 115 with 100,000 rows in 2.29s\n",
      "Memory usage: 1439.95 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 116 with 100,000 rows in 2.13s\n",
      "Memory usage: 1439.57 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 117 with 100,000 rows in 2.21s\n",
      "Memory usage: 1439.80 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 118 with 100,000 rows in 2.30s\n",
      "Memory usage: 1441.11 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 119 with 100,000 rows in 2.31s\n",
      "Memory usage: 1440.64 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 120 with 100,000 rows in 2.11s\n",
      "Memory usage: 1440.00 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 121 with 100,000 rows in 2.30s\n",
      "Memory usage: 1440.81 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 122 with 100,000 rows in 2.58s\n",
      "Memory usage: 1442.95 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 123 with 100,000 rows in 2.83s\n",
      "Memory usage: 1443.59 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 124 with 100,000 rows in 2.07s\n",
      "Memory usage: 1440.09 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 125 with 100,000 rows in 2.23s\n",
      "Memory usage: 1440.18 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 126 with 100,000 rows in 2.16s\n",
      "Memory usage: 1440.62 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 127 with 100,000 rows in 2.22s\n",
      "Memory usage: 1440.62 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 128 with 100,000 rows in 2.19s\n",
      "Memory usage: 1440.62 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 129 with 100,000 rows in 2.47s\n",
      "Memory usage: 1441.96 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 130 with 100,000 rows in 2.61s\n",
      "Memory usage: 1443.87 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 131 with 100,000 rows in 2.37s\n",
      "Memory usage: 1441.50 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 132 with 100,000 rows in 2.15s\n",
      "Memory usage: 1440.62 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 133 with 100,000 rows in 2.75s\n",
      "Memory usage: 1440.43 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 134 with 100,000 rows in 3.12s\n",
      "Memory usage: 1441.04 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 135 with 100,000 rows in 2.65s\n",
      "Memory usage: 1440.13 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 136 with 100,000 rows in 3.06s\n",
      "Memory usage: 1440.90 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 137 with 100,000 rows in 2.51s\n",
      "Memory usage: 1441.41 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 138 with 100,000 rows in 2.42s\n",
      "Memory usage: 1440.38 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 139 with 100,000 rows in 2.30s\n",
      "Memory usage: 1440.56 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 140 with 100,000 rows in 2.39s\n",
      "Memory usage: 1313.57 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 141 with 100,000 rows in 2.17s\n",
      "Memory usage: 1312.18 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 142 with 100,000 rows in 2.28s\n",
      "Memory usage: 1313.07 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 143 with 100,000 rows in 2.06s\n",
      "Memory usage: 1312.35 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 144 with 100,000 rows in 2.21s\n",
      "Memory usage: 1313.06 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 145 with 100,000 rows in 2.22s\n",
      "Memory usage: 1314.11 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 146 with 100,000 rows in 2.28s\n",
      "Memory usage: 1313.68 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 147 with 100,000 rows in 2.11s\n",
      "Memory usage: 1312.21 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 148 with 100,000 rows in 2.08s\n",
      "Memory usage: 1312.14 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 149 with 100,000 rows in 2.08s\n",
      "Memory usage: 1312.83 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 150 with 100,000 rows in 2.03s\n",
      "Memory usage: 1312.28 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 151 with 100,000 rows in 2.17s\n",
      "Memory usage: 1312.69 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 152 with 100,000 rows in 2.16s\n",
      "Memory usage: 1313.02 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 153 with 100,000 rows in 3.58s\n",
      "Memory usage: 1328.32 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 154 with 100,000 rows in 2.09s\n",
      "Memory usage: 1325.90 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 155 with 100,000 rows in 2.15s\n",
      "Memory usage: 1325.97 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 156 with 100,000 rows in 2.03s\n",
      "Memory usage: 1325.52 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 157 with 100,000 rows in 2.06s\n",
      "Memory usage: 1325.06 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 158 with 100,000 rows in 2.08s\n",
      "Memory usage: 1326.24 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 159 with 100,000 rows in 2.14s\n",
      "Memory usage: 1326.00 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 160 with 100,000 rows in 2.04s\n",
      "Memory usage: 1325.57 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 161 with 100,000 rows in 2.07s\n",
      "Memory usage: 1325.98 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 162 with 100,000 rows in 2.08s\n",
      "Memory usage: 1325.89 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 163 with 100,000 rows in 2.01s\n",
      "Memory usage: 1325.76 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 164 with 100,000 rows in 2.05s\n",
      "Memory usage: 1325.46 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 165 with 100,000 rows in 1.99s\n",
      "Memory usage: 1325.36 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 166 with 100,000 rows in 2.20s\n",
      "Memory usage: 1326.36 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 167 with 100,000 rows in 2.05s\n",
      "Memory usage: 1326.52 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 168 with 100,000 rows in 2.04s\n",
      "Memory usage: 1325.37 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 169 with 100,000 rows in 1.99s\n",
      "Memory usage: 1325.36 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 170 with 100,000 rows in 2.17s\n",
      "Memory usage: 1326.68 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 171 with 100,000 rows in 2.06s\n",
      "Memory usage: 1325.44 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 172 with 100,000 rows in 2.13s\n",
      "Memory usage: 1326.25 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 173 with 100,000 rows in 2.20s\n",
      "Memory usage: 1325.74 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 174 with 100,000 rows in 2.05s\n",
      "Memory usage: 1325.59 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 175 with 100,000 rows in 2.15s\n",
      "Memory usage: 1197.78 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 176 with 100,000 rows in 2.02s\n",
      "Memory usage: 1198.58 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 177 with 100,000 rows in 3.77s\n",
      "Memory usage: 1206.09 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 178 with 100,000 rows in 2.08s\n",
      "Memory usage: 1198.42 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 179 with 100,000 rows in 2.10s\n",
      "Memory usage: 1198.40 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 180 with 100,000 rows in 2.05s\n",
      "Memory usage: 1197.45 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 181 with 100,000 rows in 2.11s\n",
      "Memory usage: 1197.55 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 182 with 100,000 rows in 1.90s\n",
      "Memory usage: 1196.97 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 183 with 100,000 rows in 1.98s\n",
      "Memory usage: 1197.24 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 184 with 100,000 rows in 1.95s\n",
      "Memory usage: 1197.22 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 185 with 100,000 rows in 1.94s\n",
      "Memory usage: 1197.07 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 186 with 100,000 rows in 1.99s\n",
      "Memory usage: 1197.92 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 187 with 100,000 rows in 1.93s\n",
      "Memory usage: 1196.92 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 188 with 100,000 rows in 1.96s\n",
      "Memory usage: 1197.97 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 189 with 100,000 rows in 1.98s\n",
      "Memory usage: 1197.34 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 190 with 100,000 rows in 2.21s\n",
      "Memory usage: 1198.14 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 191 with 100,000 rows in 2.06s\n",
      "Memory usage: 1197.89 MB\n",
      "ğŸ“¦ Epoch 1, Chunk 192 with 29,109 rows in 0.62s\n",
      "Memory usage: 1166.47 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 1 with 100,000 rows in 4.10s\n",
      "Memory usage: 1242.90 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 2 with 100,000 rows in 3.22s\n",
      "Memory usage: 1199.16 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 3 with 100,000 rows in 2.53s\n",
      "Memory usage: 1200.48 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 4 with 100,000 rows in 2.56s\n",
      "Memory usage: 1200.34 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 5 with 100,000 rows in 2.39s\n",
      "Memory usage: 1199.30 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 6 with 100,000 rows in 2.50s\n",
      "Memory usage: 1199.66 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 7 with 100,000 rows in 2.55s\n",
      "Memory usage: 1200.12 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 8 with 100,000 rows in 2.87s\n",
      "Memory usage: 1071.90 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 9 with 100,000 rows in 3.51s\n",
      "Memory usage: 1073.67 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 10 with 100,000 rows in 3.27s\n",
      "Memory usage: 1072.62 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 11 with 100,000 rows in 2.72s\n",
      "Memory usage: 1070.62 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 12 with 100,000 rows in 2.68s\n",
      "Memory usage: 1070.85 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 13 with 100,000 rows in 2.68s\n",
      "Memory usage: 1071.48 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 14 with 100,000 rows in 2.40s\n",
      "Memory usage: 1070.48 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 15 with 100,000 rows in 2.25s\n",
      "Memory usage: 1070.77 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 16 with 100,000 rows in 2.32s\n",
      "Memory usage: 1071.55 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 17 with 100,000 rows in 2.33s\n",
      "Memory usage: 1070.98 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 18 with 100,000 rows in 2.23s\n",
      "Memory usage: 1070.02 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 19 with 100,000 rows in 2.59s\n",
      "Memory usage: 1073.11 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 20 with 100,000 rows in 2.28s\n",
      "Memory usage: 1070.48 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 21 with 100,000 rows in 2.38s\n",
      "Memory usage: 1071.37 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 22 with 100,000 rows in 2.42s\n",
      "Memory usage: 1071.14 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 23 with 100,000 rows in 2.29s\n",
      "Memory usage: 1070.62 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 24 with 100,000 rows in 2.44s\n",
      "Memory usage: 460.38 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 25 with 100,000 rows in 2.32s\n",
      "Memory usage: 459.29 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 26 with 100,000 rows in 2.50s\n",
      "Memory usage: 460.96 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 27 with 100,000 rows in 2.41s\n",
      "Memory usage: 461.78 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 28 with 100,000 rows in 2.31s\n",
      "Memory usage: 459.65 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 29 with 100,000 rows in 2.50s\n",
      "Memory usage: 460.96 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 30 with 100,000 rows in 2.30s\n",
      "Memory usage: 459.62 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 31 with 100,000 rows in 2.24s\n",
      "Memory usage: 460.75 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 32 with 100,000 rows in 2.19s\n",
      "Memory usage: 458.88 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 33 with 100,000 rows in 2.20s\n",
      "Memory usage: 459.40 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 34 with 100,000 rows in 2.25s\n",
      "Memory usage: 459.28 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 35 with 100,000 rows in 2.32s\n",
      "Memory usage: 460.17 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 36 with 100,000 rows in 2.30s\n",
      "Memory usage: 460.82 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 37 with 100,000 rows in 2.31s\n",
      "Memory usage: 460.26 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 38 with 100,000 rows in 3.30s\n",
      "Memory usage: 460.16 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 39 with 100,000 rows in 3.01s\n",
      "Memory usage: 459.97 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 40 with 100,000 rows in 2.40s\n",
      "Memory usage: 459.88 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 41 with 100,000 rows in 2.45s\n",
      "Memory usage: 460.89 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 42 with 100,000 rows in 2.45s\n",
      "Memory usage: 461.84 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 43 with 100,000 rows in 2.19s\n",
      "Memory usage: 459.30 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 44 with 100,000 rows in 2.36s\n",
      "Memory usage: 459.89 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 45 with 100,000 rows in 3.09s\n",
      "Memory usage: 457.86 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 46 with 100,000 rows in 2.86s\n",
      "Memory usage: 455.87 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 47 with 100,000 rows in 2.70s\n",
      "Memory usage: 462.46 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 48 with 100,000 rows in 2.62s\n",
      "Memory usage: 462.37 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 49 with 100,000 rows in 2.83s\n",
      "Memory usage: 463.39 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 50 with 100,000 rows in 2.19s\n",
      "Memory usage: 460.12 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 51 with 100,000 rows in 2.50s\n",
      "Memory usage: 461.69 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 52 with 100,000 rows in 2.55s\n",
      "Memory usage: 461.25 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 53 with 100,000 rows in 2.46s\n",
      "Memory usage: 461.20 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 54 with 100,000 rows in 2.34s\n",
      "Memory usage: 460.04 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 55 with 100,000 rows in 2.72s\n",
      "Memory usage: 459.91 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 56 with 100,000 rows in 2.58s\n",
      "Memory usage: 461.04 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 57 with 100,000 rows in 2.49s\n",
      "Memory usage: 460.37 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 58 with 100,000 rows in 2.60s\n",
      "Memory usage: 459.99 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 59 with 100,000 rows in 2.29s\n",
      "Memory usage: 460.98 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 60 with 100,000 rows in 2.25s\n",
      "Memory usage: 459.80 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 61 with 100,000 rows in 2.12s\n",
      "Memory usage: 459.27 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 62 with 100,000 rows in 2.23s\n",
      "Memory usage: 459.73 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 63 with 100,000 rows in 2.17s\n",
      "Memory usage: 459.86 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 64 with 100,000 rows in 2.21s\n",
      "Memory usage: 458.93 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 65 with 100,000 rows in 2.04s\n",
      "Memory usage: 459.82 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 66 with 100,000 rows in 2.03s\n",
      "Memory usage: 458.86 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 67 with 100,000 rows in 2.14s\n",
      "Memory usage: 460.29 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 68 with 100,000 rows in 2.25s\n",
      "Memory usage: 459.70 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 69 with 100,000 rows in 2.34s\n",
      "Memory usage: 459.59 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 70 with 100,000 rows in 2.18s\n",
      "Memory usage: 459.59 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 71 with 100,000 rows in 2.27s\n",
      "Memory usage: 460.02 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 72 with 100,000 rows in 2.17s\n",
      "Memory usage: 459.89 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 73 with 100,000 rows in 2.26s\n",
      "Memory usage: 459.62 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 74 with 100,000 rows in 2.12s\n",
      "Memory usage: 459.53 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 75 with 100,000 rows in 2.22s\n",
      "Memory usage: 459.82 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 76 with 100,000 rows in 2.18s\n",
      "Memory usage: 459.73 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 77 with 100,000 rows in 2.39s\n",
      "Memory usage: 460.10 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 78 with 100,000 rows in 2.20s\n",
      "Memory usage: 459.88 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 79 with 100,000 rows in 2.16s\n",
      "Memory usage: 459.45 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 80 with 100,000 rows in 2.09s\n",
      "Memory usage: 459.39 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 81 with 100,000 rows in 2.30s\n",
      "Memory usage: 461.11 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 82 with 100,000 rows in 3.49s\n",
      "Memory usage: 467.23 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 83 with 100,000 rows in 2.31s\n",
      "Memory usage: 460.27 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 84 with 100,000 rows in 2.12s\n",
      "Memory usage: 459.61 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 85 with 100,000 rows in 2.30s\n",
      "Memory usage: 460.31 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 86 with 100,000 rows in 2.19s\n",
      "Memory usage: 459.99 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 87 with 100,000 rows in 2.23s\n",
      "Memory usage: 459.71 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 88 with 100,000 rows in 2.17s\n",
      "Memory usage: 460.83 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 89 with 100,000 rows in 2.24s\n",
      "Memory usage: 459.50 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 90 with 100,000 rows in 2.13s\n",
      "Memory usage: 459.50 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 91 with 100,000 rows in 2.09s\n",
      "Memory usage: 459.60 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 92 with 100,000 rows in 2.03s\n",
      "Memory usage: 458.85 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 93 with 100,000 rows in 2.15s\n",
      "Memory usage: 459.44 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 94 with 100,000 rows in 2.21s\n",
      "Memory usage: 459.89 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 95 with 100,000 rows in 2.09s\n",
      "Memory usage: 458.89 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 96 with 100,000 rows in 2.15s\n",
      "Memory usage: 459.38 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 97 with 100,000 rows in 2.20s\n",
      "Memory usage: 459.66 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 98 with 100,000 rows in 2.40s\n",
      "Memory usage: 460.74 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 99 with 100,000 rows in 2.30s\n",
      "Memory usage: 460.60 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 100 with 100,000 rows in 2.21s\n",
      "Memory usage: 459.57 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 101 with 100,000 rows in 2.17s\n",
      "Memory usage: 459.78 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 102 with 100,000 rows in 2.17s\n",
      "Memory usage: 459.15 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 103 with 100,000 rows in 2.09s\n",
      "Memory usage: 460.65 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 104 with 100,000 rows in 2.24s\n",
      "Memory usage: 459.98 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 105 with 100,000 rows in 2.03s\n",
      "Memory usage: 458.90 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 106 with 100,000 rows in 2.17s\n",
      "Memory usage: 460.37 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 107 with 100,000 rows in 2.13s\n",
      "Memory usage: 459.66 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 108 with 100,000 rows in 2.19s\n",
      "Memory usage: 459.82 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 109 with 100,000 rows in 2.26s\n",
      "Memory usage: 460.18 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 110 with 100,000 rows in 2.04s\n",
      "Memory usage: 460.22 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 111 with 100,000 rows in 2.21s\n",
      "Memory usage: 460.43 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 112 with 100,000 rows in 2.07s\n",
      "Memory usage: 459.25 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 113 with 100,000 rows in 2.19s\n",
      "Memory usage: 460.57 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 114 with 100,000 rows in 2.08s\n",
      "Memory usage: 459.30 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 115 with 100,000 rows in 2.20s\n",
      "Memory usage: 459.28 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 116 with 100,000 rows in 2.02s\n",
      "Memory usage: 458.94 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 117 with 100,000 rows in 2.15s\n",
      "Memory usage: 459.11 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 118 with 100,000 rows in 2.25s\n",
      "Memory usage: 460.43 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 119 with 100,000 rows in 2.22s\n",
      "Memory usage: 459.91 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 120 with 100,000 rows in 2.10s\n",
      "Memory usage: 459.26 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 121 with 100,000 rows in 2.26s\n",
      "Memory usage: 459.97 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 122 with 100,000 rows in 2.52s\n",
      "Memory usage: 462.15 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 123 with 100,000 rows in 2.70s\n",
      "Memory usage: 462.71 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 124 with 100,000 rows in 2.06s\n",
      "Memory usage: 459.00 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 125 with 100,000 rows in 2.12s\n",
      "Memory usage: 458.99 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 126 with 100,000 rows in 2.11s\n",
      "Memory usage: 459.37 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 127 with 100,000 rows in 2.17s\n",
      "Memory usage: 459.58 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 128 with 100,000 rows in 2.19s\n",
      "Memory usage: 459.50 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 129 with 100,000 rows in 2.33s\n",
      "Memory usage: 460.82 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 130 with 100,000 rows in 2.68s\n",
      "Memory usage: 462.80 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 131 with 100,000 rows in 2.19s\n",
      "Memory usage: 460.26 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 132 with 100,000 rows in 2.19s\n",
      "Memory usage: 459.50 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 133 with 100,000 rows in 2.14s\n",
      "Memory usage: 459.26 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 134 with 100,000 rows in 2.09s\n",
      "Memory usage: 459.93 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 135 with 100,000 rows in 2.06s\n",
      "Memory usage: 458.89 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 136 with 100,000 rows in 2.19s\n",
      "Memory usage: 459.71 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 137 with 100,000 rows in 2.25s\n",
      "Memory usage: 460.22 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 138 with 100,000 rows in 2.14s\n",
      "Memory usage: 459.22 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 139 with 100,000 rows in 2.19s\n",
      "Memory usage: 362.70 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 140 with 100,000 rows in 2.20s\n",
      "Memory usage: 363.70 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 141 with 100,000 rows in 2.03s\n",
      "Memory usage: 362.30 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 142 with 100,000 rows in 2.20s\n",
      "Memory usage: 376.54 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 143 with 100,000 rows in 2.03s\n",
      "Memory usage: 375.83 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 144 with 100,000 rows in 2.16s\n",
      "Memory usage: 376.66 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 145 with 100,000 rows in 2.28s\n",
      "Memory usage: 377.68 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 146 with 100,000 rows in 2.37s\n",
      "Memory usage: 377.82 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 147 with 100,000 rows in 2.15s\n",
      "Memory usage: 376.29 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 148 with 100,000 rows in 2.02s\n",
      "Memory usage: 376.28 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 149 with 100,000 rows in 2.03s\n",
      "Memory usage: 376.99 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 150 with 100,000 rows in 2.07s\n",
      "Memory usage: 376.36 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 151 with 100,000 rows in 2.21s\n",
      "Memory usage: 376.63 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 152 with 100,000 rows in 2.12s\n",
      "Memory usage: 377.01 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 153 with 100,000 rows in 3.61s\n",
      "Memory usage: 380.01 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 154 with 100,000 rows in 2.03s\n",
      "Memory usage: 377.93 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 155 with 100,000 rows in 2.11s\n",
      "Memory usage: 378.14 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 156 with 100,000 rows in 1.99s\n",
      "Memory usage: 377.65 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 157 with 100,000 rows in 2.03s\n",
      "Memory usage: 377.31 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 158 with 100,000 rows in 2.13s\n",
      "Memory usage: 378.41 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 159 with 100,000 rows in 2.21s\n",
      "Memory usage: 378.20 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 160 with 100,000 rows in 2.02s\n",
      "Memory usage: 377.75 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 161 with 100,000 rows in 2.06s\n",
      "Memory usage: 378.18 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 162 with 100,000 rows in 2.14s\n",
      "Memory usage: 378.11 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 163 with 100,000 rows in 2.01s\n",
      "Memory usage: 377.94 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 164 with 100,000 rows in 2.03s\n",
      "Memory usage: 377.80 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 165 with 100,000 rows in 1.96s\n",
      "Memory usage: 377.54 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 166 with 100,000 rows in 2.25s\n",
      "Memory usage: 378.50 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 167 with 100,000 rows in 2.00s\n",
      "Memory usage: 378.69 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 168 with 100,000 rows in 1.99s\n",
      "Memory usage: 377.61 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 169 with 100,000 rows in 1.99s\n",
      "Memory usage: 377.54 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 170 with 100,000 rows in 2.18s\n",
      "Memory usage: 378.90 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 171 with 100,000 rows in 2.03s\n",
      "Memory usage: 377.57 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 172 with 100,000 rows in 2.14s\n",
      "Memory usage: 378.49 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 173 with 100,000 rows in 2.15s\n",
      "Memory usage: 377.98 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 174 with 100,000 rows in 2.01s\n",
      "Memory usage: 377.86 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 175 with 100,000 rows in 2.08s\n",
      "Memory usage: 378.00 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 176 with 100,000 rows in 1.99s\n",
      "Memory usage: 378.75 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 177 with 100,000 rows in 3.63s\n",
      "Memory usage: 386.34 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 178 with 100,000 rows in 1.97s\n",
      "Memory usage: 378.72 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 179 with 100,000 rows in 2.09s\n",
      "Memory usage: 378.66 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 180 with 100,000 rows in 1.98s\n",
      "Memory usage: 377.69 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 181 with 100,000 rows in 2.00s\n",
      "Memory usage: 377.86 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 182 with 100,000 rows in 1.93s\n",
      "Memory usage: 377.42 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 183 with 100,000 rows in 2.05s\n",
      "Memory usage: 377.68 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 184 with 100,000 rows in 2.69s\n",
      "Memory usage: 377.70 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 185 with 100,000 rows in 2.48s\n",
      "Memory usage: 377.48 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 186 with 100,000 rows in 2.42s\n",
      "Memory usage: 378.42 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 187 with 100,000 rows in 2.45s\n",
      "Memory usage: 377.32 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 188 with 100,000 rows in 2.47s\n",
      "Memory usage: 378.44 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 189 with 100,000 rows in 2.64s\n",
      "Memory usage: 377.77 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 190 with 100,000 rows in 2.72s\n",
      "Memory usage: 378.66 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 191 with 100,000 rows in 2.89s\n",
      "Memory usage: 378.41 MB\n",
      "ğŸ“¦ Epoch 2, Chunk 192 with 29,109 rows in 0.78s\n",
      "Memory usage: 346.09 MB\n",
      "âœ… Training complete in 1018.88s\n",
      "ğŸ”„ Transforming val/test data...\n",
      "ğŸ“Š Evaluating on validation set...\n",
      "Validation Accuracy: 0.2159\n",
      "ğŸ“Š Evaluating on test set...\n",
      "âœ… SGDClassifier Accuracy: 0.2140 | Time: 1018.88s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.13      1.00      0.23    284087\n",
      "           1       0.08      0.00      0.00    127503\n",
      "           2       0.28      0.00      0.01    171582\n",
      "           3       0.41      0.00      0.00    261644\n",
      "           4       0.92      0.15      0.25   1546321\n",
      "\n",
      "    accuracy                           0.21   2391137\n",
      "   macro avg       0.36      0.23      0.10   2391137\n",
      "weighted avg       0.68      0.21      0.19   2391137\n",
      "\n",
      "ğŸ’¾ Saved model to E:/amazon-sentiment-analyzer/backend/model/sgdclassifier_model\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import os\n",
    "import time\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import psutil\n",
    "import numpy as np\n",
    "\n",
    "# === Check scikit-learn version ===\n",
    "print(f\"ğŸ“¦ scikit-learn version: {sklearn.__version__}\")\n",
    "\n",
    "# === Paths ===\n",
    "data_dir = \"E:/amazon-sentiment-analyzer/backend/data/\"\n",
    "train_path = os.path.join(data_dir, \"train01.csv\")\n",
    "val_path = os.path.join(data_dir, \"val01.csv\")\n",
    "test_path = os.path.join(data_dir, \"test01.csv\")\n",
    "CHUNKSIZE = 100_000  # Suitable for 8GB RAM\n",
    "\n",
    "# === Load val/test ===\n",
    "print(\"ğŸ“‚ Loading val/test...\")\n",
    "try:\n",
    "    val_df = pd.read_csv(val_path)\n",
    "    test_df = pd.read_csv(test_path)\n",
    "    val_df['text'] = val_df['text'].fillna(\"\").astype(str).str.lower()  # Basic preprocessing\n",
    "    test_df['text'] = test_df['text'].fillna(\"\").astype(str).str.lower()\n",
    "    print(f\"Memory usage after loading: {psutil.Process().memory_info().rss / 1024**2:.2f} MB\")\n",
    "except FileNotFoundError as e:\n",
    "    raise FileNotFoundError(f\"File not found: {e}\")\n",
    "\n",
    "# === Compute Class Weights ===\n",
    "print(\"ğŸ” Computing class weights from sample...\")\n",
    "sample_size = 200_000  # Increased for better estimation\n",
    "y_sample = []\n",
    "reader = pd.read_csv(train_path, chunksize=CHUNKSIZE, nrows=sample_size)\n",
    "for chunk in reader:\n",
    "    try:\n",
    "        chunk['label'] = pd.to_numeric(chunk['label'], errors='coerce')\n",
    "        chunk = chunk.dropna(subset=['label'])\n",
    "        chunk['label'] = chunk['label'].astype(int)\n",
    "        y_sample.extend(chunk['label'].tolist())\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error processing chunk for class weights: {e}\")\n",
    "if not y_sample:\n",
    "    raise ValueError(\"No valid labels found in sample. Check 'label' column in train01.csv.\")\n",
    "\n",
    "# Inspect unique labels\n",
    "unique_labels = np.unique(y_sample)\n",
    "print(f\"Unique labels in sample: {unique_labels}\")\n",
    "if len(unique_labels) < 2:\n",
    "    raise ValueError(f\"Expected at least 2 classes, found {len(unique_labels)}: {unique_labels}\")\n",
    "\n",
    "# Compute class weights\n",
    "try:\n",
    "    class_weights = compute_class_weight('balanced', classes=unique_labels, y=y_sample)\n",
    "    class_weight_dict = {unique_labels[i]: class_weights[i] for i in range(len(unique_labels))}\n",
    "    print(f\"Class weights: {class_weight_dict}\")\n",
    "except ValueError as e:\n",
    "    print(f\"âš ï¸ Error computing class weights: {e}. Using equal weights.\")\n",
    "    class_weight_dict = {label: 1.0 for label in unique_labels}\n",
    "\n",
    "# === Initialize Vectorizer ===\n",
    "print(\"ğŸ”  Initializing HashingVectorizer...\")\n",
    "vectorizer = HashingVectorizer(n_features=50_000, norm='l2', alternate_sign=False)  # Increased features\n",
    "print(\"âœ… Vectorizer ready.\")\n",
    "\n",
    "# === Initialize Model ===\n",
    "model = SGDClassifier(\n",
    "    loss='log_loss',\n",
    "    max_iter=30,  # Increased for better convergence\n",
    "    learning_rate='adaptive',\n",
    "    eta0=0.001,  # Lowered for stability\n",
    "    alpha=0.0001,  # Adjusted regularization\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# === Train Model Incrementally ===\n",
    "print(\"ğŸš€ Training SGDClassifier on chunks...\")\n",
    "start = time.time()\n",
    "for epoch in range(2):  # Two passes over data\n",
    "    reader = pd.read_csv(train_path, chunksize=CHUNKSIZE)\n",
    "    for i, chunk in enumerate(reader):\n",
    "        chunk_start = time.time()\n",
    "        try:\n",
    "            chunk['text'] = chunk['text'].fillna(\"\").astype(str).str.lower()\n",
    "            chunk['label'] = pd.to_numeric(chunk['label'], errors='coerce')\n",
    "            chunk = chunk.dropna(subset=['label'])\n",
    "            chunk['label'] = chunk['label'].astype(int)\n",
    "            X_chunk = vectorizer.transform(chunk['text'])\n",
    "            y_chunk = chunk['label']\n",
    "            valid_idx = y_chunk.isin(class_weight_dict.keys())\n",
    "            if not valid_idx.all():\n",
    "                print(f\"âš ï¸ Chunk {i+1} contains invalid labels. Filtering...\")\n",
    "                X_chunk = X_chunk[valid_idx]\n",
    "                y_chunk = y_chunk[valid_idx]\n",
    "            if len(y_chunk) == 0:\n",
    "                print(f\"âš ï¸ Chunk {i+1} has no valid labels. Skipping.\")\n",
    "                continue\n",
    "            model.partial_fit(X_chunk, y_chunk, classes=unique_labels, sample_weight=[class_weight_dict[y] for y in y_chunk])\n",
    "            print(f\"ğŸ“¦ Epoch {epoch+1}, Chunk {i+1} with {len(chunk):,} rows in {time.time() - chunk_start:.2f}s\")\n",
    "            print(f\"Memory usage: {psutil.Process().memory_info().rss / 1024**2:.2f} MB\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Error in epoch {epoch+1}, chunk {i+1}: {e}\")\n",
    "train_time = time.time() - start\n",
    "print(f\"âœ… Training complete in {train_time:.2f}s\")\n",
    "\n",
    "# === Transform and Evaluate Val/Test ===\n",
    "print(\"ğŸ”„ Transforming val/test data...\")\n",
    "X_val = vectorizer.transform(val_df['text'])\n",
    "X_test = vectorizer.transform(test_df['text'])\n",
    "y_val = pd.to_numeric(val_df['label'], errors='coerce').astype(int)\n",
    "y_test = pd.to_numeric(test_df['label'], errors='coerce').astype(int)\n",
    "\n",
    "print(\"ğŸ“Š Evaluating on validation set...\")\n",
    "y_val_pred = model.predict(X_val)\n",
    "val_acc = accuracy_score(y_val, y_val_pred)\n",
    "print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "print(\"ğŸ“Š Evaluating on test set...\")\n",
    "y_pred = model.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"âœ… SGDClassifier Accuracy: {acc:.4f} | Time: {train_time:.2f}s\")\n",
    "print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "# === Save Model and Vectorizer ===\n",
    "model_dir = \"E:/amazon-sentiment-analyzer/backend/model/sgdclassifier_model\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "joblib.dump(model, os.path.join(model_dir, \"model.pkl\"), compress=3)\n",
    "joblib.dump(vectorizer, os.path.join(model_dir, \"vectorizer.pkl\"), compress=3)\n",
    "print(f\"ğŸ’¾ Saved model to {model_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
